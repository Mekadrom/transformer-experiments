tokenizer_run_name: BPE

d_model: 6144
n_gqa_groups: 8
n_heads: 8
d_queries: 128
d_values: 128
d_inner: 15360
use_infinite_attention: false
infinite_attention_n_segments: 1
use_moe: false
n_experts: 0
moe_top_k: 0
moe_diversity_loss_coefficient: 0.0
moe_diversity_inclusion_epoch: 0
n_decoder_layers: 64
dropout: 0.2
norm_type: layer
norm_eps: 0.00001

maxlen: 4096
train_dataset: bigcode/the-stack-dedup
tie_embeddings: true

init_weights_from: glorot_uniform
init_weights_gain: 1.0
use_admin: false

positional_encoding_type: rotary
positional_encoding_dim: 128
learnable_positional_encoding: false

decoder_param_sharing_type: cycle-rev
m_decoder_independent_layers: 16
activation_function: swiglu
latent_repr_type: none

batch_size: 4
n_steps: 100000
warmup_steps: 8000
beta1: 0.9
beta2: 0.98
epsilon: 1.e-9
label_smoothing: 0.1
clip_grad_norm: 1.0

prune_mode: none
prune_type: all
prune_structured: false
prune_heads_amount: 0.0
prunt_heads_norm: 2
prune_ffn_amount: 0.0
prune_ffn_norm: 2
n_prune_retrains: 2
prune_retrain_n_steps: 10000
prune_retrain_warmup_steps: 800

distillation_teacher_run_name:

start_epoch: 0
print_frequency: 20
device: cuda
save_initial_checkpoint: false
cudnn_benchmark: false
detect_nans: false

torch_compile_model: false
dynamo_cache_size_limit: 16

early_stop: true
early_stop_patience: 3
early_stop_min_delta: 0.01
early_stop_num_latest_checkpoints_to_avg: 3
